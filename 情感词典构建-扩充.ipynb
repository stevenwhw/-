{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文金融情绪词典构建流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import uuid\n",
    "import requests\n",
    "import hashlib\n",
    "import time\n",
    "from imp import reload\n",
    "import json\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "from multiprocessing import Process, Manager,Pool\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义使用有道智云翻译单词的函数\n",
    "reload(sys)\n",
    "\n",
    "YOUDAO_URL = 'https://openapi.youdao.com/api'\n",
    "APP_KEY = '1b06ba3dbabdc53d'\n",
    "APP_SECRET = '0ZzWnSfmcgrEpcldvcufautHXjwGIu5r'\n",
    "\n",
    "\n",
    "def encrypt(signStr):\n",
    "    hash_algorithm = hashlib.sha256()\n",
    "    hash_algorithm.update(signStr.encode('utf-8'))\n",
    "    return hash_algorithm.hexdigest()\n",
    "\n",
    "\n",
    "def truncate(q):\n",
    "    if q is None:\n",
    "        return None\n",
    "    size = len(q)\n",
    "    return q if size <= 20 else q[0:10] + str(size) + q[size - 10:size]\n",
    "\n",
    "\n",
    "def do_request(data):\n",
    "    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "    return requests.post(YOUDAO_URL, data=data, headers=headers)\n",
    "\n",
    "\n",
    "# def connect(word,fromlanguage,tolanguage):\n",
    "def connect(word):\n",
    "    try:\n",
    "        q = word\n",
    "\n",
    "        data = {}\n",
    "        #设置翻译语种\n",
    "        data['from'] = 'en'\n",
    "        data['to'] = 'zh-CHS'\n",
    "        data['signType'] = 'v3'\n",
    "        curtime = str(int(time.time()))\n",
    "        data['curtime'] = curtime\n",
    "        salt = str(uuid.uuid1())\n",
    "        signStr = APP_KEY + truncate(q) + salt + curtime + APP_SECRET\n",
    "        sign = encrypt(signStr)\n",
    "        data['appKey'] = APP_KEY\n",
    "        data['q'] = q\n",
    "        data['salt'] = salt\n",
    "        data['sign'] = sign\n",
    "        # data['vocabId'] = \"您的用户词表ID\"\n",
    "\n",
    "        response = do_request(data)\n",
    "        contentType = response.headers['Content-Type']\n",
    "        if contentType == \"audio/mp3\":\n",
    "            millis = int(round(time.time() * 1000))\n",
    "            filePath = \"合成的音频存储路径\" + str(millis) + \".mp3\"\n",
    "            fo = open(filePath, 'wb')\n",
    "            fo.write(response.content)\n",
    "            fo.close()\n",
    "        else:\n",
    "            # print(response.content)\n",
    "            # return response.content\n",
    "            return json.loads(response.content.decode())['translation'][0]\n",
    "    except:\n",
    "        # print(word+'返回翻译失败')\n",
    "        return np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取LM情感词典的单词\n",
    "LMdict=pd.read_csv('Loughran-McDonald_MasterDictionary_1993-2021.csv',encoding='utf-8')\n",
    "LMdictWordlist=LMdict['Word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:41<00:00,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Word Translation\n",
      "0            AARDVARK          土豚\n",
      "1           AARDVARKS          土豚\n",
      "2               ABACI          算盘\n",
      "3               ABACK           吓\n",
      "4              ABACUS          算盘\n",
      "..                ...         ...\n",
      "995    ADMINISTRATION          政府\n",
      "996   ADMINISTRATIONS          政府\n",
      "997    ADMINISTRATIVE          行政\n",
      "998  ADMINISTRATIVELY          管理\n",
      "999     ADMINISTRATOR         管理员\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#LM词典翻译为中文(单进程)\n",
    "TranslatedWord=[]\n",
    "for word in tqdm(LMdictWordlist[:1000]):\n",
    "    t=connect(word)\n",
    "    TranslatedWord.append([word,t])\n",
    "TranslatedWord=pd.DataFrame(TranslatedWord,columns=['Word','Translation'])\n",
    "print(TranslatedWord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "监视进度:: 100%|██████████| 1000/1000 [00:31<00:00, 32.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Word Translation\n",
      "0            AARDVARK          土豚\n",
      "1           AARDVARKS          土豚\n",
      "2               ABACI          算盘\n",
      "3               ABACK           吓\n",
      "4              ABACUS          算盘\n",
      "5            ABACUSES          算盘\n",
      "6               ABAFT         在船尾\n",
      "7             ABALONE          鲍鱼\n",
      "8            ABALONES          鲍鱼\n",
      "9             ABANDON          放弃\n",
      "10          ABANDONED        被遗弃的\n",
      "11         ABANDONING          放弃\n",
      "12        ABANDONMENT          放弃\n",
      "13       ABANDONMENTS          放弃\n",
      "14           ABANDONS         抛弃了\n",
      "15              ABASE          自卑\n",
      "16             ABASED         自卑的\n",
      "17          ABASEMENT          降低\n",
      "18         ABASEMENTS          降低\n",
      "19             ABASES          打倒\n",
      "20              ABASH       使局促不安\n",
      "21            ABASHED         尴尬的\n",
      "22          ABASHEDLY         尴尬的\n",
      "23            ABASHES       使局促不安\n",
      "24           ABASHING       使局促不安\n",
      "25          ABASHMENT          羞愧\n",
      "26         ABASHMENTS          羞愧\n",
      "27            ABASING          贬低\n",
      "28              ABATE          减弱\n",
      "29             ABATED          减弱\n",
      "..                ...         ...\n",
      "970     ADJUSTABILITY         适应性\n",
      "971        ADJUSTABLE          可调\n",
      "972       ADJUSTABLES          可调\n",
      "973          ADJUSTED          调整\n",
      "974          ADJUSTER         调整器\n",
      "975         ADJUSTERS         理赔员\n",
      "976         ADJUSTING          调整\n",
      "977        ADJUSTMENT          调整\n",
      "978       ADJUSTMENTS          调整\n",
      "979          ADJUSTOR         调节器\n",
      "980         ADJUSTORS         调节器\n",
      "981           ADJUSTS          调整\n",
      "982          ADJUTANT          副官\n",
      "983         ADJUTANTS          副官\n",
      "984          ADJUVANT          辅助\n",
      "985             ADMAN         广告商\n",
      "986             ADMEN         广告人\n",
      "987        ADMINISTER          管理\n",
      "988      ADMINISTERED          管理\n",
      "989     ADMINISTERING          管理\n",
      "990       ADMINISTERS          管理\n",
      "991      ADMINISTRATE          管理\n",
      "992     ADMINISTRATED          管理\n",
      "993     ADMINISTRATES          管理\n",
      "994    ADMINISTRATING          管理\n",
      "995    ADMINISTRATION          政府\n",
      "996   ADMINISTRATIONS          政府\n",
      "997    ADMINISTRATIVE          行政\n",
      "998  ADMINISTRATIVELY          管理\n",
      "999     ADMINISTRATOR         管理员\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#读LM词典翻译为中文（多进程）\n",
    "TranslatedWord=pd.DataFrame()\n",
    "if __name__ == '__main__':\n",
    "    with Pool(5) as p:\n",
    "        t=LMdictWordlist[:1000]\n",
    "        res=list(tqdm(p.imap(connect, t),total=len(t),desc='监视进度:'))\n",
    "        # res=p.map(connect, LMdictWordlist[:10])\n",
    "        wordlist=[]\n",
    "        for j in range(len(t)):\n",
    "            wordlist.append([t[j],res[j]])\n",
    "        TranslatedWord=pd.DataFrame(wordlist,columns=['Word','Translation'])\n",
    "    print(TranslatedWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "监视进度:: 100%|██████████| 10000/10000 [08:58<00:00, 18.57it/s]  \n",
      "监视进度:: 100%|██████████| 10000/10000 [08:10<00:00, 16.89it/s] \n",
      "监视进度:: 100%|██████████| 10000/10000 [08:29<00:00, 19.62it/s]  \n",
      "监视进度:: 100%|██████████| 10000/10000 [11:12<00:00, 14.87it/s] \n",
      "监视进度:: 100%|██████████| 10000/10000 [08:15<00:00, 20.19it/s] \n",
      "监视进度:: 100%|██████████| 10000/10000 [08:23<00:00, 19.85it/s] \n",
      "监视进度:: 100%|██████████| 10000/10000 [07:59<00:00, 20.84it/s] \n",
      "监视进度:: 100%|██████████| 10000/10000 [08:23<00:00, 19.87it/s]  \n",
      "监视进度:: 100%|██████████| 6531/6531 [06:10<00:00, 17.64it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#读取LM词典转化为中文（多进程）\n",
    "TranslatedWord=pd.DataFrame()\n",
    "if __name__ == '__main__':\n",
    "    for i in range(int(len(LMdictWordlist)/10000)+1):    \n",
    "        with Pool(5) as p:\n",
    "            if (i+1)*10000>len(LMdictWordlist):\n",
    "                t=LMdictWordlist[i*10000:]\n",
    "            else:\n",
    "                t=LMdictWordlist[i*10000:(i+1)*10000]\n",
    "            res=list(tqdm(p.imap(connect, t),total=len(t),desc='监视进度:'))\n",
    "            # res=p.map(connect, LMdictWordlist[:10])\n",
    "            wordlist=[]\n",
    "            for j in range(len(t)):\n",
    "                wordlist.append([t[j],res[j]])\n",
    "            TranslatedWord=pd.DataFrame(wordlist,columns=['Word','Translation'])\n",
    "        TranslatedWord.to_excel('Translation'+str(i)+'.xlsx',encoding='utf-8')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#合并所有翻译词到原表格\n",
    "TranslatedWord=pd.DataFrame()\n",
    "for i in range(9):\n",
    "    t=pd.read_excel('情绪词典翻译结果/Translation'+str(i)+'.xlsx')\n",
    "    TranslatedWord=TranslatedWord.append(t)\n",
    "TranslatedWord=TranslatedWord.reset_index()[['Word','Translation']]\n",
    "TranslatedWord=pd.merge(LMdict,TranslatedWord,how='left')\n",
    "TranslatedWord.to_csv('情绪词典.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "开始分词：: 100%|██████████| 118398/118398 [23:50<00:00, 82.76it/s]  \n",
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_5972/2243661295.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  t['分词']=t.progress_apply(lambda x:split(str(x['正文'])), axis=1)\n",
      "开始分词：: 100%|██████████| 118398/118398 [31:21<00:00, 62.92it/s] \n",
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_5972/2243661295.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  t['分词']=t.progress_apply(lambda x:split(str(x['正文'])), axis=1)\n",
      "开始分词：: 100%|██████████| 118398/118398 [33:34<00:00, 58.77it/s] \n",
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_5972/2243661295.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  t['分词']=t.progress_apply(lambda x:split(str(x['正文'])), axis=1)\n",
      "开始分词：: 100%|██████████| 118398/118398 [23:54<00:00, 82.54it/s] \n",
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_5972/2243661295.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  t['分词']=t.progress_apply(lambda x:split(str(x['正文'])), axis=1)\n",
      "开始分词：: 100%|██████████| 118395/118395 [22:24<00:00, 88.08it/s] \n",
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_5972/2243661295.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  t['分词']=t.progress_apply(lambda x:split(str(x['正文'])), axis=1)\n"
     ]
    }
   ],
   "source": [
    "#加载停用词\n",
    "with open('hit_stopwords.txt',encoding='utf-8') as f:\n",
    "    words=f.readlines()\n",
    "    sw=[i.replace('\\n','') for i in words]\n",
    "    sw.append(' ')\n",
    "    sw.append('\\n')\n",
    "\n",
    "#生成大小写字母和数字\n",
    "chars=[]\n",
    "for i in range(97,123):\n",
    "    chars.append(chr(i))\n",
    "for i in range(65,91):\n",
    "    chars.append(chr(i))\n",
    "for i in range(10):\n",
    "    chars.append(str(i))\n",
    "\n",
    "#判断词语是否包含数字或字母\n",
    "def isNumorAlphain(word):\n",
    "    for i in chars:\n",
    "        if i in word:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "#分词函数\n",
    "def split(t): \n",
    "    words = pseg.cut(t,use_paddle=True)\n",
    "    tempwords=[]\n",
    "    for word,flag in words:\n",
    "        if word not in set(sw) and isNumorAlphain(word)==0:\n",
    "            tempwords.append(word)\n",
    "    return tempwords\n",
    "\n",
    "#读取新闻文本\n",
    "#开始分词\n",
    "#pandas==0.24.0 tqdm==4.32.2\n",
    "texts=pd.read_csv('news_all_company_ture.csv',encoding='utf-8')\n",
    "texts['正文']=texts['正文'].astype(str)\n",
    "# texts['分词']=texts.apply(lambda x:split(x['正文']),axis=1)\n",
    "#分五组，避免表格过大\n",
    "for i in range(0, len(texts), math.ceil(len(texts)/5)):\n",
    "        if i+math.ceil(len(texts)/5)>len(texts):\n",
    "            t=texts[i:]\n",
    "            tqdm.pandas(desc=\"开始分词：\")\n",
    "            t['分词']=t.progress_apply(lambda x:split(str(x['正文'])), axis=1)\n",
    "            t.to_csv('分词/'+str(i)+'.csv',encoding='utf-8-sig')\n",
    "        else:\n",
    "            t=texts[i:i+math.ceil(len(texts)/5)]\n",
    "            tqdm.pandas(desc=\"开始分词：\")\n",
    "            t['分词']=t.progress_apply(lambda x:split(str(x['正文'])), axis=1)\n",
    "            t.to_csv('分词/'+str(i)+'.csv',encoding='utf-8-sig')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词频统计，调整词频表，生成word2vec模型预训练文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:57<00:00, 11.48s/it]\n"
     ]
    }
   ],
   "source": [
    "#读取所有文本，进行词频统计\n",
    "textsname=os.listdir('分词/')\n",
    "\n",
    "#运用Counter计算词频的函数\n",
    "def count_key_value(corpus):\n",
    "    word_freq = Counter(chain(*corpus))\n",
    "    return word_freq\n",
    "\n",
    "#读取所有分词结果，进行词频统计\n",
    "allwordfreq=pd.DataFrame()\n",
    "for i in tqdm(textsname):\n",
    "    corpus=pd.read_csv('分词/'+i)['分词'].tolist()[:]\n",
    "    corpus=[corpus[c][2:-2].split(\"', '\") for c in range(len(corpus))]\n",
    "    tempwordfreq=count_key_value(corpus)\n",
    "    tempwordfreq=pd.DataFrame([[key,value] for key,value in tempwordfreq.items()],columns=['word','times'])\n",
    "    allwordfreq=allwordfreq.append(tempwordfreq)\n",
    "\n",
    "allwordfreq=allwordfreq.groupby('word').apply(lambda x:sum(x['times']))\n",
    "allwordfreq=pd.DataFrame({\"word\":allwordfreq.index,\"times\":allwordfreq.values})\n",
    "allwordfreq.to_excel('初步分词词频表.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word  times\n",
      "0        一一二     46\n",
      "1        一一例      1\n",
      "2       一一列举      9\n",
      "3        一一对      2\n",
      "4       一一对应     49\n",
      "...      ...    ...\n",
      "360654    龟鹿      3\n",
      "360655   龟鹿丹      1\n",
      "360656   龟鹿宁      1\n",
      "360657    龟龄    187\n",
      "360658     龠      2\n",
      "\n",
      "[360659 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#根据初步的词频表，机器加上人工审核去除特殊字符和不合理的词语，整理得到较为干净的语料准备进行word2vec训练\n",
    "allwordfreq=pd.read_excel('初步分词词频表.xlsx')\n",
    "allwordfreq['word']=allwordfreq['word'].astype(str)\n",
    "\n",
    "#去除包含非中文字符部分\n",
    "def is_chinese(word):\n",
    "    if word == '':\n",
    "        return 0\n",
    "    for i in word:\n",
    "        if i < '\\u4e00' or i > '\\u9fa5':\n",
    "            return 0\n",
    "    return 1\n",
    "\n",
    "allwordfreq['is_chinese']=allwordfreq.apply(lambda x:is_chinese(x['word']),axis=1)\n",
    "allwordfreq=allwordfreq[allwordfreq.is_chinese!=0][['word','times']].dropna().reset_index()\n",
    "del allwordfreq['index']\n",
    "print(allwordfreq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word  times\n",
      "0        一一二     46\n",
      "1       一一列举      9\n",
      "2       一一对应     49\n",
      "3         一丁     11\n",
      "4        一丁点      8\n",
      "...      ...    ...\n",
      "140919    龟山      9\n",
      "140920    龟甲     28\n",
      "140921    龟苓      6\n",
      "140922   龟苓膏     67\n",
      "140923    龟龄    187\n",
      "\n",
      "[140924 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#删除词频小于5的词\n",
    "allwordfreq=allwordfreq[allwordfreq.times>5].reset_index()\n",
    "del allwordfreq['index']\n",
    "print(allwordfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word  times\n",
      "0        一一二     46\n",
      "1       一一列举      9\n",
      "2       一一对应     49\n",
      "3         一丁     11\n",
      "4        一丁点      8\n",
      "...      ...    ...\n",
      "137057    龟山      9\n",
      "137058    龟甲     28\n",
      "137059    龟苓      6\n",
      "137060   龟苓膏     67\n",
      "137061    龟龄    187\n",
      "\n",
      "[137062 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#删除单个字\n",
    "def is_single(word):\n",
    "    if len(word)==1:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "allwordfreq['is_single']=allwordfreq.apply(lambda x:is_single(x['word']),axis=1)\n",
    "allwordfreq=allwordfreq[allwordfreq.is_single==0][['word','times']].dropna().reset_index()\n",
    "del allwordfreq['index']\n",
    "print(allwordfreq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#更细致的可以考虑删除叠词、根据词性筛选、进行命名实体识别\n",
    "\n",
    "#保存筛选后的词频表\n",
    "allwordfreq.to_csv('筛选后词频表.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用set数据类型加快判断\n",
    "allwordfreq=set(allwordfreq['word'].drop_duplicates().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118398/118398 [00:04<00:00, 24675.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118398/118398 [00:07<00:00, 16507.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118398/118398 [00:08<00:00, 14634.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118398/118398 [00:05<00:00, 19838.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118395/118395 [00:04<00:00, 25639.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#根据数据清理后的词频表，生成进行word2vec的训练的语料\n",
    "word2vec_corpus=[]\n",
    "for i in os.listdir('分词'):\n",
    "    t=pd.read_csv('分词/'+i)\n",
    "    for text in tqdm(t['分词'].tolist()):\n",
    "        words=[]\n",
    "        for word in text[2:-2].split(\"', '\"):\n",
    "            if word in allwordfreq:\n",
    "                words.append(word)\n",
    "        # print(words)\n",
    "        word2vec_corpus.append(words)\n",
    "    print(len(word2vec_corpus))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec训练\n",
    "model=gensim.models.Word2Vec(word2vec_corpus,vector_size=300,window=5,min_count=5,sg=1,alpha=0.0005,epochs=20,workers=multiprocessing.cpu_count())\n",
    "#word2vec模型保存\n",
    "model.save('金融情绪词典word2vec_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('科研', 0.7208125591278076), ('产品开发', 0.7160720229148865), ('研制', 0.7090649008750916), ('产业化', 0.6690207123756409), ('技术开发', 0.6645838618278503), ('投入', 0.6575615406036377), ('技术创新', 0.6550519466400146), ('市场推广', 0.6383605003356934), ('核心技术', 0.6297745704650879), ('自主', 0.6281081438064575)]\n"
     ]
    }
   ],
   "source": [
    "#读取训练模型结果\n",
    "model=gensim.models.Word2Vec.load('金融情绪词典word2vec_model.model')\n",
    "#训练结果展示\n",
    "print(model.wv.most_similar('研发',topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42741/42741 [01:36<00:00, 442.51it/s] \n"
     ]
    }
   ],
   "source": [
    "#生成与词典中最相似的top5词典\n",
    "df = []\n",
    "for i in tqdm(pd.read_csv('情绪词典.csv')['Translation'].drop_duplicates().tolist()):\n",
    "    try:\n",
    "        res=model.wv.most_similar(i,topn=5)\n",
    "        for j in res:\n",
    "            df.append([i,j[0],j[1]])\n",
    "    except:\n",
    "        pass\n",
    "df=pd.DataFrame(df,columns=['','seedword','word','similarity'])\n",
    "df.to_csv('情绪词典_Word2vec_top5.csv',encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bae86fdb662cc19355e8ff56ce9a1889c7beba2d924e0f82725aa45d10dd43de"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
